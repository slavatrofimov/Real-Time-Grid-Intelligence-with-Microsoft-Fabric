{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# üöÄ Fabric Solution Accelerator Deployment Notebook\n",
    "\n",
    "This notebook orchestrates the end-to-end deployment of **Advanced Metering Infrastructure (AMI)** solution assets into the current Microsoft Fabric workspace using the `fabric-cicd` library.\n",
    "\n",
    "## This notebook performs the following tasks:\n",
    "1. **üì¶ Package Installation**: Install required libraries and dependencies\n",
    "1. **‚öôÔ∏è Parameter Configuration and Library Import:** Configure parameters and import required libraries\n",
    "1. **üì• Source Code Download**: Download and extracts solution content from GitHub repository\n",
    "1. **üöÄ Fabric Item Deployment**: Deploy Fabric items and map them to each other to preserve dependencies\n",
    "1. **‚úÖ Post-Deployment Tasks**: Complete post-deployment configuration tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74892b-1f56-4f03-bec5-80cd71c75171",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "## üì¶ Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install fabric-cicd --quiet\n",
    "%pip install --upgrade azure-core azure-identity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c28fcb-e7ed-4cdd-ab50-b0440674bd24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è Restarting Python kernel for installed packages to take effect\")\n",
    "notebookutils.session.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdacf15e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Parameter Configuration and Library Import\n",
    "\n",
    "Update these values to customize the deployment for your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd895b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Define user-configurable parameters\n",
    "DEFAULT_API_ROOT_URL = \"https://api.fabric.microsoft.com\" #Default is https://api.fabric.microsoft.com, but may vary depending on your environment\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42758a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# The following settings should not be modified by the user\n",
    "# GitHub Repository Configuration\n",
    "repo_owner = \"slavatrofimov\"\n",
    "repo_name = \"Real-Time-Grid-Intelligence-with-Microsoft-Fabric\"\n",
    "branch = \"main\"\n",
    "folder_to_extract = \"workspace\"\n",
    "\n",
    "# GitHub Personal Access Token (if required for private repositories)\n",
    "github_token = \"\"\n",
    "\n",
    "# Deployment Configuration\n",
    "deployment_environment = \"DEV\"  # Options: DEV, TEST, PROD\n",
    "\n",
    "# File System Paths\n",
    "path_prefix = '.lakehouse/default/Files'\n",
    "\n",
    "extract_to_directory = path_prefix + \"/src\"\n",
    "repository_directory = extract_to_directory + \"/\" + folder_to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any\n",
    "from datetime import datetime, timezone\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "import fabric_cicd.constants\n",
    "from fabric_cicd import FabricWorkspace, publish_all_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## üì• Source Code Download\n",
    "\n",
    "Download and extract solution source files and configuration from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Download and extract GitRepository to a folder\n",
    "\n",
    "def download_and_extract_folder(repo_owner, repo_name, github_token, extract_to, branch=\"main\", \n",
    "                               folder_to_extract=\"workspace\", remove_folder_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Download a GitHub repository and extract a specific folder directly to disk\n",
    "    without saving the zip file.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub repository owner\n",
    "        repo_name: GitHub repository name\n",
    "        github_token: GitHub personal access token (if required)\n",
    "        extract_to: Local directory to extract files to\n",
    "        branch: Git branch to download (default: \"main\")\n",
    "        folder_to_extract: Folder path within the repo to extract\n",
    "        remove_folder_prefix: Prefix to remove from extracted file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "        url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "\n",
    "        # Set up headers for authentication if a token is provided\n",
    "        headers = {}\n",
    "        if github_token:\n",
    "            headers['Authorization'] = f\"token {github_token}\"\n",
    "            headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n",
    "\n",
    "        # Make a request to the GitHub API\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Delete target directory if exists\n",
    "        if os.path.exists(extract_to) and os.path.isdir(extract_to):\n",
    "            shutil.rmtree(extract_to)\n",
    "            print(f'Deleted existing directory: {extract_to}')\n",
    "        \n",
    "        # Ensure the extraction directory exists\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        \n",
    "        # Process the zip file directly from memory\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                # Check if the file is in the folder we want to extract\n",
    "                normalized_path = re.sub(r'^.*?/', '/', file_info.filename)\n",
    "                \n",
    "                if normalized_path.startswith(f\"/{folder_to_extract}\"):\n",
    "                    # Calculate the output path\n",
    "                    parts = file_info.filename.split('/')\n",
    "                    relative_path = '/'.join(parts[1:])  # Remove repo root folder\n",
    "                    \n",
    "                    # Remove the specified prefix if provided\n",
    "                    if remove_folder_prefix:\n",
    "                        relative_path = relative_path.replace(remove_folder_prefix, \"\", 1)\n",
    "                    \n",
    "                    output_path = os.path.join(extract_to, relative_path)\n",
    "                    \n",
    "                    # Skip if it's a directory entry\n",
    "                    if file_info.filename.endswith('/'):\n",
    "                        os.makedirs(output_path, exist_ok=True)\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure the directory for the file exists\n",
    "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                    \n",
    "                    # Extract and write the file\n",
    "                    with zipf.open(file_info) as source_file:\n",
    "                        with open(output_path, 'wb') as target_file:\n",
    "                            target_file.write(source_file.read())\n",
    "                            \n",
    "        print(f\"Successfully extracted {folder_to_extract} from {repo_owner}/{repo_name} to {extract_to}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"A {type(e).__name__} error occurred. This error may be intermittent. Consider stopping the current notebook session and re-running the notebook again.\"\n",
    "        print(error_msg)\n",
    "        #  Re-raise the exception \n",
    "        raise\n",
    "\n",
    "# Execute repo download and extraction using configured parameters\n",
    "print(f\"üì• Downloading {repo_name} from {repo_owner}/{repo_name}:{branch}\")\n",
    "print(f\"üìÅ Extracting '{folder_to_extract}' folder to '{extract_to_directory}'\")\n",
    "\n",
    "download_and_extract_folder(\n",
    "    repo_owner=repo_owner,\n",
    "    repo_name=repo_name,\n",
    "    github_token=github_token,\n",
    "    extract_to=extract_to_directory,\n",
    "    branch=branch,\n",
    "    folder_to_extract=folder_to_extract,\n",
    "    remove_folder_prefix=\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Source code download and extraction completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5584266",
   "metadata": {},
   "source": [
    "## üöÄ Fabric Item Deployment\n",
    "\n",
    "Deploy solution assets to the current Fabric workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddd135-e093-4f9f-a24c-eca43c3845a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Token Credential class for authentication in a Fabric Notebook\n",
    "# This class enables the fabric-cicd library to be run in a Fabric notebook\n",
    "\n",
    "class FabricNotebookTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Notebooks using notebookutils authentication.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        \"\"\"Get access token from Fabric notebook environment.\"\"\"\n",
    "        access_token = notebookutils.credentials.getToken(\"pbi\")       \n",
    "        expiration = self._extract_jwt_expiration(access_token)\n",
    "        return AccessToken(token=access_token, expires_on=expiration)\n",
    "    \n",
    "    def _extract_jwt_expiration(self, token: str) -> int:\n",
    "        \"\"\"Extract expiration timestamp from JWT token.\"\"\"\n",
    "        try:\n",
    "            # Split JWT and get payload (middle part)\n",
    "            payload_b64 = token.split(\".\")[1]           \n",
    "            # Add padding if needed for base64 decoding\n",
    "            payload_b64 += \"=\" * (-len(payload_b64) % 4)\n",
    "            # Decode and parse payload\n",
    "            payload_bytes = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\"))\n",
    "            payload = json.loads(payload_bytes.decode(\"utf-8\"))\n",
    "            # Extract expiration claim\n",
    "            exp = payload.get(\"exp\")\n",
    "            if exp is None:\n",
    "                raise ValueError(\"JWT missing expiration claim\")\n",
    "            return exp\n",
    "        except (IndexError, json.JSONDecodeError, ValueError) as e:\n",
    "            raise ValueError(f\"Invalid JWT token format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d3d8d-c1a5-4a0e-9faf-656f32c26974",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Configure constants for fabric-cicd library\n",
    "fabric_cicd.constants.DEFAULT_API_ROOT_URL = DEFAULT_API_ROOT_URL\n",
    "\n",
    "# Get current workspace information\n",
    "client = fabric.FabricRestClient()\n",
    "workspace_id = fabric.get_workspace_id()\n",
    "print(f\"Target workspace ID: {workspace_id}\")\n",
    "\n",
    "# Enable debugging for more verbose execution logging\n",
    "if DEBUG:\n",
    "    from fabric_cicd import change_log_level\n",
    "    change_log_level(\"DEBUG\")\n",
    "\n",
    "# Function to execute deployment of all specified item types\n",
    "def deploy_artifacts(target_workspace):\n",
    "    print(\"üöÄ Starting deployment of Fabric items...\")\n",
    "    print(f\"üìã Item types in scope: {', '.join(target_workspace.item_type_in_scope)}\")\n",
    "    publish_all_items(target_workspace)\n",
    "    print(\"‚úÖ Deployment completed successfully!\")\n",
    "\n",
    "# Initialize the FabricWorkspace object with configured parameters\n",
    "target_workspace = FabricWorkspace(\n",
    "    workspace_id=workspace_id,\n",
    "    environment=deployment_environment,\n",
    "    repository_directory=repository_directory,\n",
    "    token_credential=FabricNotebookTokenCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead30a2-a6fb-4d84-8cb6-25c2407b08ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Start by deploying data stores\n",
    "item_types_in_scope = [\n",
    "    \"Eventhouse\", \n",
    "    \"KQLDatabase\", \n",
    "    \"Lakehouse\"\n",
    "]\n",
    "\n",
    "target_workspace.item_type_in_scope=item_types_in_scope\n",
    "\n",
    "# Execute deployment of all specified item types\n",
    "deploy_artifacts(target_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4a4a-80dd-4ab9-8f7a-eb0b0446734b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy remaining items\n",
    "item_types_in_scope = [\n",
    "    \"Eventstream\", \n",
    "    \"Notebook\", \n",
    "    \"KQLDashboard\", \n",
    "    \"SemanticModel\", \n",
    "    \"Report\", \n",
    "    \"Reflex\", \n",
    "    \"DataAgent\"\n",
    "]\n",
    "\n",
    "target_workspace.item_type_in_scope=item_types_in_scope\n",
    "\n",
    "# Execute deployment of all specified item types\n",
    "deploy_artifacts(target_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ‚úÖ Post-Deployment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109221e-9fef-4ef0-b311-d3c0cd396514",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### üîß Automated Post-Deployment Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a0ff2-310a-441c-b4b8-fe8e2d35087e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Move the KQL Database to the same subfolder as the Eventhouse\n",
    "# To compensate for the unexpected placement of the item in the initial deployment\n",
    "\n",
    "# Get current workspace information\n",
    "client = fabric.FabricRestClient()\n",
    "workspace_id = fabric.get_workspace_id()\n",
    "eventhouse_id = fabric.resolve_item_id('PowerUtilitiesEH', 'Eventhouse')\n",
    "\n",
    "# Step 1: move eventhose to the root folder\n",
    "url = f'v1/workspaces/{workspace_id}/items/{eventhouse_id}/move'\n",
    "\n",
    "# Define payload for the API call and execute the call\n",
    "payload = {} # Move to root folder\n",
    "client.post(url, json = payload)\n",
    "\n",
    "# Step 2: move eventhose (with the child KQL database) to the desired destination folder\n",
    "# Define a function to get the id of the destination folder\n",
    "def get_folder_id_by_name(folder_name: str, workspace_id: str) -> str | None:\n",
    "    \"\"\"Get folder ID by display name, returns None if not found.\"\"\"\n",
    "    try:\n",
    "\n",
    "        url = f'v1/workspaces/{workspace_id}/folders'\n",
    "        folders = client.get(url)\n",
    "        for folder in folders.json()[\"value\"]:\n",
    "            if folder[\"displayName\"] == 'Store and Query':\n",
    "                return(folder[\"id\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get target folder\n",
    "target_folder_id = get_folder_id_by_name('Store and Query', workspace_id)\n",
    "\n",
    "# Define payload for the API call and execute the call\n",
    "payload = {\n",
    "  \"targetFolderId\": f\"{target_folder_id}\"\n",
    "}\n",
    "\n",
    "client.post(url, json = payload)\n",
    "print(\"‚úÖ Eventhouse moved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8991e4-bf92-4463-b88e-3df37cf79cf0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Download and save sample data files from the GitHub repository to the lakehouse\n",
    "\n",
    "# Get abfs path to the RfeferenceDataLH lakehouse and mount it as a local filesystem\n",
    "abfsPath = notebookutils.lakehouse.getWithProperties('ReferenceDataLH').properties['abfsPath']\n",
    "notebookutils.fs.mount(abfsPath,\"/ReferenceDataLH\") # Mount \n",
    "\n",
    "# Define the path to the local directory where the file will be saved\n",
    "target_directory = notebookutils.fs.getMountPath(\"/ReferenceDataLH\") + \"/Files/data/\"\n",
    "\n",
    "# Function to download a file from GitHub repository and save it locally\n",
    "def download_file(repo_owner, repo_name, github_token, branch, file_path, target_directory):\n",
    "    \"\"\"\n",
    "    Download and save a file from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub repository owner\n",
    "        repo_name: GitHub repository name\n",
    "        github_token: GitHub personal access token (if required)\n",
    "        branch: Git branch to download (default: \"main\")\n",
    "        file_path: File path within the repo to download\n",
    "        target_directory: Directory where to save the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Download sample data file\n",
    "    file_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/refs/heads/{branch}/{file_path}\"\n",
    "    file_name = file_url.split(\"/\")[-1]  # Extract filename from URL\n",
    "\n",
    "    try:\n",
    "        # Create target directory if it doesn't exist\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "        \n",
    "        # Set up headers for authentication if a token is provided\n",
    "        headers = {}\n",
    "        if github_token:\n",
    "            headers['Authorization'] = f\"token {github_token}\"\n",
    "            headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n",
    "\n",
    "        # Download the file\n",
    "        print(f\"üì• Downloading file from {file_url}\")\n",
    "        response = requests.get(file_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to target directory\n",
    "        target_path = os.path.join(target_directory, file_name)\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"‚úÖ File saved successfully to {target_path}\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the path to the desired file in the repo\n",
    "file_path = \"data/vehicle_route_points.json\"\n",
    "download_file(repo_owner, repo_name, github_token, branch, file_path, target_directory)\n",
    "\n",
    "# Define the path to the desired file in the repo\n",
    "file_path = \"data/transmission_lines.geojson\"\n",
    "download_file(repo_owner, repo_name, github_token, branch, file_path, target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### ‚öôÔ∏è Manual Post-Deployment Steps\n",
    "Complete the following tasks to finish the installation\n",
    "1. Generate reference data by running the **AMI Reference Data Simulation** notebook (in the Simulation folder)\n",
    "1. Configure credentials for semantic models in the \"Visualize and Chat\" folder\n",
    "1. Perform a one-time manual refresh of semantic models"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
