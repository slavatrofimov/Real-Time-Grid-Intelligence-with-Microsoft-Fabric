{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# üöÄ Fabric Solution Accelerator Deployment Notebook\n",
    "\n",
    "This notebook orchestrates the end-to-end deployment of **Advanced Metering Infrastructure (AMI)** solution assets into the current Microsoft Fabric workspace using the `fabric-cicd` library.\n",
    "\n",
    "## This notebook performs the following tasks:\n",
    "1. **üì¶ Package Installation**: Install required libraries and dependencies\n",
    "1. **‚öôÔ∏è Parameter Configuration and Library Import:** Configure parameters and import required libraries\n",
    "1. **üì• Source Code Download**: Download and extracts solution content from GitHub repository\n",
    "1. **üöÄ Fabric Item Deployment**: Deploy Fabric items and map them to each other to preserve dependencies\n",
    "1. **‚úÖ Post-Deployment Tasks**: Complete post-deployment configuration tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74892b-1f56-4f03-bec5-80cd71c75171",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "## üì¶ Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install fabric-cicd --quiet\n",
    "%pip install --upgrade azure-core azure-identity --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c28fcb-e7ed-4cdd-ab50-b0440674bd24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è Restarting Python kernel for installed packages to take effect\")\n",
    "notebookutils.session.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdacf15e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Parameter Configuration and Library Import\n",
    "\n",
    "Update these values to customize the deployment for your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd895b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Define user-configurable parameters\n",
    "DEFAULT_API_ROOT_URL = \"https://api.fabric.microsoft.com\" #Default is https://api.fabric.microsoft.com, but may vary depending on your environment\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42758a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# The following settings should not be modified by the user\n",
    "# GitHub Repository Configuration\n",
    "repo_owner = \"slavatrofimov\"\n",
    "repo_name = \"Real-Time-Grid-Intelligence-with-Microsoft-Fabric\"\n",
    "branch = \"main\"\n",
    "folder_to_extract = \"workspace\"\n",
    "\n",
    "# GitHub Personal Access Token (if required for private repositories)\n",
    "github_token = \"\"\n",
    "\n",
    "# Deployment Configuration\n",
    "deployment_environment = \"DEV\"  # Options: DEV, TEST, PROD\n",
    "\n",
    "# File System Paths\n",
    "path_prefix = '.lakehouse/default/Files'\n",
    "\n",
    "extract_to_directory = path_prefix + \"/src\"\n",
    "repository_directory = extract_to_directory + \"/\" + folder_to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Optional, Any\n",
    "from datetime import datetime, timezone\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "import fabric_cicd.constants\n",
    "from fabric_cicd import FabricWorkspace, publish_all_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## üì• Source Code Download\n",
    "\n",
    "Download and extract solution source files and configuration from the GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Download and extract GitRepository to a folder\n",
    "\n",
    "def download_and_extract_folder(repo_owner, repo_name, github_token, extract_to, branch=\"main\", \n",
    "                               folder_to_extract=\"workspace\", remove_folder_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Download a GitHub repository and extract a specific folder directly to disk\n",
    "    without saving the zip file.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub repository owner\n",
    "        repo_name: GitHub repository name\n",
    "        github_token: GitHub personal access token (if required)\n",
    "        extract_to: Local directory to extract files to\n",
    "        branch: Git branch to download (default: \"main\")\n",
    "        folder_to_extract: Folder path within the repo to extract\n",
    "        remove_folder_prefix: Prefix to remove from extracted file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "        url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "\n",
    "        # Set up headers for authentication if a token is provided\n",
    "        headers = {}\n",
    "        if github_token:\n",
    "            headers['Authorization'] = f\"token {github_token}\"\n",
    "            headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n",
    "\n",
    "        # Make a request to the GitHub API\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Delete target directory if exists\n",
    "        if os.path.exists(extract_to) and os.path.isdir(extract_to):\n",
    "            shutil.rmtree(extract_to)\n",
    "            print(f'Deleted existing directory: {extract_to}')\n",
    "        \n",
    "        # Ensure the extraction directory exists\n",
    "        os.makedirs(extract_to, exist_ok=True)\n",
    "        \n",
    "        # Process the zip file directly from memory\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                # Check if the file is in the folder we want to extract\n",
    "                normalized_path = re.sub(r'^.*?/', '/', file_info.filename)\n",
    "                \n",
    "                if normalized_path.startswith(f\"/{folder_to_extract}\"):\n",
    "                    # Calculate the output path\n",
    "                    parts = file_info.filename.split('/')\n",
    "                    relative_path = '/'.join(parts[1:])  # Remove repo root folder\n",
    "                    \n",
    "                    # Remove the specified prefix if provided\n",
    "                    if remove_folder_prefix:\n",
    "                        relative_path = relative_path.replace(remove_folder_prefix, \"\", 1)\n",
    "                    \n",
    "                    output_path = os.path.join(extract_to, relative_path)\n",
    "                    \n",
    "                    # Skip if it's a directory entry\n",
    "                    if file_info.filename.endswith('/'):\n",
    "                        os.makedirs(output_path, exist_ok=True)\n",
    "                        continue\n",
    "                    \n",
    "                    # Ensure the directory for the file exists\n",
    "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                    \n",
    "                    # Extract and write the file\n",
    "                    with zipf.open(file_info) as source_file:\n",
    "                        with open(output_path, 'wb') as target_file:\n",
    "                            target_file.write(source_file.read())\n",
    "                            \n",
    "        print(f\"Successfully extracted {folder_to_extract} from {repo_owner}/{repo_name} to {extract_to}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"A {type(e).__name__} error occurred. This error may be intermittent. Consider stopping the current notebook session and re-running the notebook again.\"\n",
    "        print(error_msg)\n",
    "        #  Re-raise the exception \n",
    "        raise\n",
    "\n",
    "# Execute repo download and extraction using configured parameters\n",
    "print(f\"üì• Downloading {repo_name} from {repo_owner}/{repo_name}:{branch}\")\n",
    "print(f\"üìÅ Extracting '{folder_to_extract}' folder to '{extract_to_directory}'\")\n",
    "\n",
    "download_and_extract_folder(\n",
    "    repo_owner=repo_owner,\n",
    "    repo_name=repo_name,\n",
    "    github_token=github_token,\n",
    "    extract_to=extract_to_directory,\n",
    "    branch=branch,\n",
    "    folder_to_extract=folder_to_extract,\n",
    "    remove_folder_prefix=\"\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Source code download and extraction completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5584266",
   "metadata": {},
   "source": [
    "## üöÄ Fabric Item Deployment\n",
    "\n",
    "Deploy solution assets to the current Fabric workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddd135-e093-4f9f-a24c-eca43c3845a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Token Credential class for authentication in a Fabric Notebook\n",
    "# This class enables the fabric-cicd library to be run in a Fabric notebook\n",
    "\n",
    "class FabricNotebookTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Notebooks using notebookutils authentication.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        \"\"\"Get access token from Fabric notebook environment.\"\"\"\n",
    "        access_token = notebookutils.credentials.getToken(\"pbi\")       \n",
    "        expiration = self._extract_jwt_expiration(access_token)\n",
    "        return AccessToken(token=access_token, expires_on=expiration)\n",
    "    \n",
    "    def _extract_jwt_expiration(self, token: str) -> int:\n",
    "        \"\"\"Extract expiration timestamp from JWT token.\"\"\"\n",
    "        try:\n",
    "            # Split JWT and get payload (middle part)\n",
    "            payload_b64 = token.split(\".\")[1]           \n",
    "            # Add padding if needed for base64 decoding\n",
    "            payload_b64 += \"=\" * (-len(payload_b64) % 4)\n",
    "            # Decode and parse payload\n",
    "            payload_bytes = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\"))\n",
    "            payload = json.loads(payload_bytes.decode(\"utf-8\"))\n",
    "            # Extract expiration claim\n",
    "            exp = payload.get(\"exp\")\n",
    "            if exp is None:\n",
    "                raise ValueError(\"JWT missing expiration claim\")\n",
    "            return exp\n",
    "        except (IndexError, json.JSONDecodeError, ValueError) as e:\n",
    "            raise ValueError(f\"Invalid JWT token format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d3d8d-c1a5-4a0e-9faf-656f32c26974",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Configure constants for fabric-cicd library\n",
    "fabric_cicd.constants.DEFAULT_API_ROOT_URL = DEFAULT_API_ROOT_URL\n",
    "\n",
    "# Get current workspace information\n",
    "client = fabric.FabricRestClient()\n",
    "workspace_id = fabric.get_workspace_id()\n",
    "print(f\"Target workspace ID: {workspace_id}\")\n",
    "\n",
    "# Enable debugging for more verbose execution logging\n",
    "if DEBUG:\n",
    "    from fabric_cicd import change_log_level\n",
    "    change_log_level(\"DEBUG\")\n",
    "\n",
    "# Function to execute deployment of all specified item types\n",
    "def deploy_artifacts(target_workspace):\n",
    "    print(\"üöÄ Starting deployment of Fabric items...\")\n",
    "    print(f\"üìã Item types in scope: {', '.join(target_workspace.item_type_in_scope)}\")\n",
    "    publish_all_items(target_workspace)\n",
    "    print(\"‚úÖ Deployment completed successfully!\")\n",
    "\n",
    "# Initialize the FabricWorkspace object with configured parameters\n",
    "target_workspace = FabricWorkspace(\n",
    "    workspace_id=workspace_id,\n",
    "    environment=deployment_environment,\n",
    "    repository_directory=repository_directory,\n",
    "    token_credential=FabricNotebookTokenCredential()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead30a2-a6fb-4d84-8cb6-25c2407b08ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Start by deploying data stores\n",
    "item_types_in_scope = [\n",
    "    \"Eventhouse\", \n",
    "    \"KQLDatabase\", \n",
    "    \"Lakehouse\"\n",
    "]\n",
    "\n",
    "target_workspace.item_type_in_scope=item_types_in_scope\n",
    "\n",
    "# Execute deployment of all specified item types\n",
    "deploy_artifacts(target_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4a4a-80dd-4ab9-8f7a-eb0b0446734b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy remaining items\n",
    "item_types_in_scope = [\n",
    "    \"Eventstream\", \n",
    "    \"Notebook\", \n",
    "    \"KQLDashboard\", \n",
    "    \"SemanticModel\", \n",
    "    \"Report\", \n",
    "    \"Reflex\", \n",
    "    \"DataAgent\"\n",
    "]\n",
    "\n",
    "target_workspace.item_type_in_scope=item_types_in_scope\n",
    "\n",
    "# Execute deployment of all specified item types\n",
    "deploy_artifacts(target_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738dc8e",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Deploy Service Area Map\n",
    "Deploy the Service Area Map with artifact ID remapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924027ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Service Area Map with artifact ID remapping\n",
    "import time\n",
    "\n",
    "# Define a function to get the id of the destination folder\n",
    "def get_folder_id_by_name(folder_name: str, workspace_id: str) -> str | None:\n",
    "    \"\"\"Get folder ID by display name, returns None if not found.\"\"\"\n",
    "    try:\n",
    "\n",
    "        url = f'v1/workspaces/{workspace_id}/folders'\n",
    "        folders = client.get(url)\n",
    "        for folder in folders.json()[\"value\"]:\n",
    "            if folder[\"displayName\"] == folder_name:\n",
    "                return(folder[\"id\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_item_definition_from_repo(item_relative_path):\n",
    "    \"\"\"Load item definition files from the extracted repository.\"\"\"\n",
    "    item_path = os.path.join(repository_directory, item_relative_path)\n",
    "    \n",
    "    # Read .platform file for metadata\n",
    "    platform_file = os.path.join(item_path, '.platform')\n",
    "    with open(platform_file, 'r', encoding='utf-8') as f:\n",
    "        platform_data = json.load(f)\n",
    "    \n",
    "    return platform_data, item_path\n",
    "\n",
    "\n",
    "def remap_map_artifact_ids(map_definition, workspace_id):\n",
    "    \"\"\"\n",
    "    Remap artifact IDs in the map definition to match the current workspace.\n",
    "    Updates lakehouses, kqlDatabases, and parentArtifactIds in layer sources.\n",
    "    \"\"\"\n",
    "    print(\"üîß Remapping artifact IDs in map definition...\")\n",
    "    \n",
    "    # Build artifact ID mapping\n",
    "    artifact_mapping = {}\n",
    "    \n",
    "    # Remap lakehouses\n",
    "    if 'lakehouses' in map_definition.get('dataSources', {}):\n",
    "        for lakehouse in map_definition['dataSources']['lakehouses']:\n",
    "            old_artifact_id = lakehouse['artifactId']\n",
    "            \n",
    "            # Get the actual lakehouse ID in this workspace\n",
    "            try:\n",
    "                new_artifact_id = fabric.resolve_item_id('ReferenceDataLH', 'Lakehouse')\n",
    "                artifact_mapping[old_artifact_id] = new_artifact_id\n",
    "                \n",
    "                # Update the map definition\n",
    "                lakehouse['workspaceId'] = workspace_id\n",
    "                lakehouse['artifactId'] = new_artifact_id\n",
    "                print(f\"  ‚úì Remapped Lakehouse: {old_artifact_id} ‚Üí {new_artifact_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Warning: Could not resolve Lakehouse ID: {e}\")\n",
    "    \n",
    "    # Remap KQL Databases\n",
    "    if 'kqlDataBases' in map_definition.get('dataSources', {}):\n",
    "        for kql_db in map_definition['dataSources']['kqlDataBases']:\n",
    "            old_artifact_id = kql_db['artifactId']\n",
    "            \n",
    "            # Get the actual KQL Database ID in this workspace\n",
    "            try:\n",
    "                new_artifact_id = fabric.resolve_item_id('PowerUtilitiesEH', 'KQLDatabase')\n",
    "                artifact_mapping[old_artifact_id] = new_artifact_id\n",
    "                \n",
    "                # Update the map definition\n",
    "                kql_db['workspaceId'] = workspace_id\n",
    "                kql_db['artifactId'] = new_artifact_id\n",
    "                print(f\"  ‚úì Remapped KQL Database: {old_artifact_id} ‚Üí {new_artifact_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Warning: Could not resolve KQL Database ID: {e}\")\n",
    "    \n",
    "    # Remap parentArtifactIds in layer sources\n",
    "    if 'layerSources' in map_definition:\n",
    "        for layer_source in map_definition['layerSources']:\n",
    "            if 'parentArtifactId' in layer_source and layer_source['parentArtifactId']:\n",
    "                old_parent_id = layer_source['parentArtifactId']\n",
    "                \n",
    "                # Look up the new ID from our mapping\n",
    "                if old_parent_id in artifact_mapping:\n",
    "                    new_parent_id = artifact_mapping[old_parent_id]\n",
    "                    layer_source['parentArtifactId'] = new_parent_id\n",
    "                    print(f\"  ‚úì Remapped Layer Source '{layer_source['name']}': {old_parent_id} ‚Üí {new_parent_id}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Warning: No mapping found for parentArtifactId in layer '{layer_source['name']}': {old_parent_id}\")\n",
    "    \n",
    "    print(f\"‚úÖ Artifact ID remapping completed ({len(artifact_mapping)} mappings)\")\n",
    "    return map_definition\n",
    "\n",
    "\n",
    "def deploy_map_item(map_name, map_relative_path):\n",
    "    \"\"\"\n",
    "    Deploy a Map item using the Fabric REST API with artifact ID remapping.\n",
    "    Reference: https://learn.microsoft.com/en-us/rest/api/fabric/map/items/create-map\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìç Deploying Map: {map_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Get map definition from repository\n",
    "        platform_data, map_path = get_item_definition_from_repo(map_relative_path)\n",
    "        \n",
    "        display_name = platform_data['metadata']['displayName']\n",
    "        description = platform_data['metadata'].get('description', '')\n",
    "        \n",
    "        # Read map.json file\n",
    "        map_json_file = os.path.join(map_path, 'map.json')\n",
    "        with open(map_json_file, 'r', encoding='utf-8') as f:\n",
    "            map_definition = json.load(f)\n",
    "        \n",
    "        # Remap artifact IDs to match current workspace\n",
    "        map_definition = remap_map_artifact_ids(map_definition, workspace_id)\n",
    "        \n",
    "        # Step 1: Create the Map item\n",
    "        print(f\"üîß Creating Map item: {display_name}\")\n",
    "        create_url = f'v1/workspaces/{workspace_id}/maps'\n",
    "        \n",
    "        create_payload = {\n",
    "            \"displayName\": display_name,\n",
    "            \"description\": description\n",
    "        }\n",
    "        \n",
    "        create_response = client.post(create_url, json=create_payload)\n",
    "        \n",
    "        if create_response.status_code in [200, 201]:\n",
    "            map_item = create_response.json()\n",
    "            map_id = map_item['id']\n",
    "            print(f\"‚úÖ Map item created successfully (ID: {map_id})\")\n",
    "        elif create_response.status_code == 409:\n",
    "            print(f\"‚ÑπÔ∏è Map '{display_name}' already exists, retrieving existing item...\")\n",
    "            map_id = fabric.resolve_item_id(display_name, 'Map')\n",
    "            print(f\"‚úÖ Using existing Map (ID: {map_id})\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to create Map: {create_response.status_code} - {create_response.text}\")\n",
    "        \n",
    "        # Step 2: Update Map definition with remapped artifact IDs\n",
    "        print(f\"üîß Updating Map definition with remapped artifact IDs...\")\n",
    "        update_url = f'v1/workspaces/{workspace_id}/maps/{map_id}/updateDefinition'\n",
    "        \n",
    "        # Prepare definition payload according to API spec\n",
    "        definition_payload = {\n",
    "            \"definition\": {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"path\": \"map.json\",\n",
    "                        \"payload\": base64.b64encode(json.dumps(map_definition).encode('utf-8')).decode('utf-8'),\n",
    "                        \"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        update_response = client.post(update_url, json=definition_payload)\n",
    "        \n",
    "        if update_response.status_code in [200, 202]:\n",
    "            print(f\"‚úÖ Map definition updated successfully\")\n",
    "            \n",
    "            # If async operation, poll for completion\n",
    "            if update_response.status_code == 202:\n",
    "                print(f\"‚è≥ Waiting for definition update to complete...\")\n",
    "                time.sleep(5)  # Give it time to process\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: Map definition update returned status {update_response.status_code}\")\n",
    "            print(f\"Response: {update_response.text[:500]}\")\n",
    "        \n",
    "        # Step 3: Move to correct folder\n",
    "        target_folder_id = get_folder_id_by_name('Visualize and Chat', workspace_id)\n",
    "        if target_folder_id:\n",
    "            print(f\"üîß Moving Map to 'Visualize and Chat' folder...\")\n",
    "            move_url = f'v1/workspaces/{workspace_id}/items/{map_id}/move'\n",
    "            move_payload = {\"targetFolderId\": target_folder_id}\n",
    "            move_response = client.post(move_url, json=move_payload)\n",
    "            \n",
    "            if move_response.status_code == 200:\n",
    "                print(f\"‚úÖ Map moved to folder successfully\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not move Map to folder: {move_response.status_code}\")\n",
    "        \n",
    "        print(f\"‚úÖ Map deployment completed: {display_name}\")\n",
    "        return map_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deploying Map: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Deploy the Service Area Map\n",
    "try:\n",
    "    map_id = deploy_map_item(\n",
    "        map_name=\"Service Area Map\",\n",
    "        map_relative_path=\"Visualize and Chat/Service Area Map.Map\"\n",
    "    )    \n",
    "    print(\"‚úÖ Service Area Map deployed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Map deployment failed: {e}\")\n",
    "    print(\"‚ÑπÔ∏è Check the logs above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## ‚úÖ Post-Deployment Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109221e-9fef-4ef0-b311-d3c0cd396514",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### üîß Automated Post-Deployment Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a0ff2-310a-441c-b4b8-fe8e2d35087e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Move the KQL Database to the same subfolder as the Eventhouse\n",
    "# To compensate for the unexpected placement of the item in the initial deployment\n",
    "\n",
    "# Get current workspace information\n",
    "client = fabric.FabricRestClient()\n",
    "workspace_id = fabric.get_workspace_id()\n",
    "eventhouse_id = fabric.resolve_item_id('PowerUtilitiesEH', 'Eventhouse')\n",
    "\n",
    "# Step 1: move eventhose to the root folder\n",
    "url = f'v1/workspaces/{workspace_id}/items/{eventhouse_id}/move'\n",
    "\n",
    "# Define payload for the API call and execute the call\n",
    "payload = {} # Move to root folder\n",
    "client.post(url, json = payload)\n",
    "\n",
    "# Step 2: move eventhose (with the child KQL database) to the desired destination folder\n",
    "# Get target folder\n",
    "target_folder_id = get_folder_id_by_name('Store and Query', workspace_id)\n",
    "\n",
    "# Define payload for the API call and execute the call\n",
    "payload = {\n",
    "  \"targetFolderId\": f\"{target_folder_id}\"\n",
    "}\n",
    "\n",
    "client.post(url, json = payload)\n",
    "print(\"‚úÖ Eventhouse moved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8991e4-bf92-4463-b88e-3df37cf79cf0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Download and save sample data files from the GitHub repository to the lakehouse\n",
    "\n",
    "# Get abfs path to the RfeferenceDataLH lakehouse and mount it as a local filesystem\n",
    "abfsPath = notebookutils.lakehouse.getWithProperties('ReferenceDataLH').properties['abfsPath']\n",
    "notebookutils.fs.mount(abfsPath,\"/ReferenceDataLH\") # Mount \n",
    "\n",
    "# Define the path to the local directory where the file will be saved\n",
    "target_directory = notebookutils.fs.getMountPath(\"/ReferenceDataLH\") + \"/Files/data/\"\n",
    "\n",
    "# Function to download a file from GitHub repository and save it locally\n",
    "def download_file(repo_owner, repo_name, github_token, branch, file_path, target_directory):\n",
    "    \"\"\"\n",
    "    Download and save a file from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub repository owner\n",
    "        repo_name: GitHub repository name\n",
    "        github_token: GitHub personal access token (if required)\n",
    "        branch: Git branch to download (default: \"main\")\n",
    "        file_path: File path within the repo to download\n",
    "        target_directory: Directory where to save the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Download sample data file\n",
    "    file_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/refs/heads/{branch}/{file_path}\"\n",
    "    file_name = file_url.split(\"/\")[-1]  # Extract filename from URL\n",
    "\n",
    "    try:\n",
    "        # Create target directory if it doesn't exist\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "        \n",
    "        # Set up headers for authentication if a token is provided\n",
    "        headers = {}\n",
    "        if github_token:\n",
    "            headers['Authorization'] = f\"token {github_token}\"\n",
    "            headers[\"Accept\"] = \"application/vnd.github.v3+json\"\n",
    "\n",
    "        # Download the file\n",
    "        print(f\"üì• Downloading file from {file_url}\")\n",
    "        response = requests.get(file_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to target directory\n",
    "        target_path = os.path.join(target_directory, file_name)\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"‚úÖ File saved successfully to {target_path}\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Define the paths to the desired files in the repo\n",
    "file_paths = [\"data/vehicle_route_points.json\", \"data/transmission_lines.geojson\"]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    download_file(repo_owner, repo_name, github_token, branch, file_path, target_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### ‚öôÔ∏è Manual Post-Deployment Steps\n",
    "Complete the following tasks to finish the installation\n",
    "1. Refresh your browser window to reflect the newly-deployed Fabric items\n",
    "1. Generate reference data by opening and running the **AMI Reference Data Simulation** notebook (in the Simulation folder)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
